"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.ParallelFileWriter = void 0;
const fs = require("fs");
const path_1 = require("path");
const stream_1 = require("stream");
const AsyncLock = require("async-lock");
const LRUCache = require("lru-cache");
const mkdirp = require("mkdirp");
const rdf_serialize_1 = require("rdf-serialize");
/**
 * A parallel file writer enables the writing to an infinite number of files in parallel.
 *
 * It works around system I/O limitations regarding the maximum number of open file descriptors
 * by internally only having a certain number of open file streams,
 * and intelligently closing/re-opening streams when needed using an LRU strategy.
 */
class ParallelFileWriter {
    constructor(options) {
        this.cache = new LRUCache({
            max: options.streams,
            dispose: (key, value) => this.closeWriteEntry(key, value),
            noDisposeOnSet: true,
        });
        this.lock = new AsyncLock();
        this.fileClosingPromises = [];
    }
    /**
     * Get a write stream that accepts RDF/JS quads for the given file path.
     * It will automatically be serialized to the RDF format of the given content type.
     *
     * The returned stream is only safe to use until another call to this method.
     *
     * This is safe with regards to non-existing folders.
     * If any of the parent folders do not exist, they will be created.
     *
     * @param path Path to the file to write to.
     * @param contentType The content type to serialize for.
     *                    Note that this only should be content types that enable appending
     */
    getWriteStream(path, contentType) {
        return __awaiter(this, void 0, void 0, function* () {
            return this.lock.acquire('getWriteStream', () => this.getWriteStreamUnsafe(path, contentType));
        });
    }
    getWriteStreamUnsafe(path, contentType) {
        return __awaiter(this, void 0, void 0, function* () {
            // Try to get the stream from cache, or open a new one if not yet open.
            let writeEntry = this.cache.get(path);
            if (!writeEntry) {
                // Before opening new streams, wait for previous file closings to end
                yield Promise.all(this.fileClosingPromises);
                this.fileClosingPromises = [];
                // Open the file stream, and prepare the RDF serializer
                const writeStream = new stream_1.PassThrough({ objectMode: true });
                const folder = (0, path_1.dirname)(path);
                yield mkdirp(folder);
                const fileStream = fs.createWriteStream(path, { flags: 'a' });
                rdf_serialize_1.default.serialize(writeStream, { contentType }).pipe(fileStream);
                writeEntry = { writeStream, fileStream };
                this.cache.set(path, writeEntry);
            }
            return writeEntry.writeStream;
        });
    }
    /**
     * Close all open streams.
     */
    close() {
        return __awaiter(this, void 0, void 0, function* () {
            // Add listeners to be able to await stream closing
            const outputStreamPromises = this.cache.keys()
                .map(key => this.cache.get(key).fileStream)
                .map(fileStream => new Promise((resolve, reject) => {
                fileStream.on('finish', resolve);
                fileStream.on('error', reject);
            }));
            // Close all output streams
            this.cache.forEach(writeEntry => writeEntry.writeStream.end());
            // Wait for all streams to close
            yield Promise.all(outputStreamPromises);
        });
    }
    closeWriteEntry(path, writeEntry) {
        this.fileClosingPromises.push(new Promise((resolve, reject) => {
            writeEntry.fileStream.on('finish', resolve);
            writeEntry.fileStream.on('error', reject);
        }));
        writeEntry.writeStream.end();
    }
}
exports.ParallelFileWriter = ParallelFileWriter;
//# sourceMappingURL=ParallelFileWriter.js.map